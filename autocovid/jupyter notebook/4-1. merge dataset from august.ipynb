{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge dataset from august\n",
    "\n",
    "- 8월 이후로 수집된 데이터를 모두 합하고 필요하지 않은 columns 삭제\n",
    "- 활동일을 정규화, 중간 파일을 `data\\extracted\\merged_route_check_01.csv`, `data\\extracted\\merged_route_check_02.csv`로 저장\n",
    "- `merged_route_check_02.csv`파일 수작업으로 데이터 처리 후 환자 정보 없는 데이터는 `merged_route_check_03_01.csv`로, 환자 정보를 가진 데이터는 `merged_route_check_03_02.csv`로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path(os.getcwd()).parent.parent\n",
    "sys.path.append(str(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "import re\n",
    "\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8월 이후 수집한 데이터 목록 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndataset_path = join(root, 'data', 'extracted')\\ndataset_list = [f for f in listdir(dataset_path) if isfile(join(dataset_path, f))]\\ndataset_list\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "dataset_path = join(root, 'data', 'extracted')\n",
    "dataset_list = [f for f in listdir(dataset_path) if isfile(join(dataset_path, f))]\n",
    "dataset_list\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f05ba24c809e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdata_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilename_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'from'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[a-z]*(?=\\d)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mdata_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "filename_list = ['dobong1.csv', 'dongdaemoon1.csv', 'dongdaemoon2.csv',\n",
    "                'dongjak1.csv', 'eunpyeong1.csv', 'gangbuk1.csv',\n",
    "                'gangdong1.csv', 'gangdong2.csv', 'gangnam1.csv',\n",
    "                'gangnam2.csv', 'gangnam3.csv', 'gangseo1.csv',\n",
    "                'gangseo2.csv', 'gangseo3.csv', 'gangseo4.csv',\n",
    "                'gangseo5.csv', 'guro1.csv', 'gwanak1.csv',\n",
    "                'gwanak2.csv', 'jongno1.csv', 'jung1.csv',\n",
    "                'jungnang1.csv', 'jungnang2.csv', 'jungnang3.csv',\n",
    "                'mapo1.csv', 'mapo2.csv', 'nowon1.csv',\n",
    "                'seocho1.csv', 'seodaemoon1.csv', 'seongbuk1.csv',\n",
    "                'seongdong1.csv', 'seongdong2.csv', 'songpa1.csv',\n",
    "                'yeongdeungpo1.csv', 'yongsan1.csv']\n",
    "file_path = join(root, 'data', 'extracted')\n",
    "\n",
    "data_list = []\n",
    "for filename in filename_list:\n",
    "    data = pd.read_csv(join(file_path, filename))\n",
    "    data['from'] = re.search(r'[a-z]*(?=\\d)', filename).group()\n",
    "    data_list.append(data)\n",
    "    \n",
    "merged_data = pd.concat(data_list, axis=0)\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "column 통일 methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_nan(row, column):\n",
    "    if row[column] == np.nan: return True\n",
    "    else: return False\n",
    "    \n",
    "def merge_column(df, target_column, column_list):\n",
    "    for column in column_list:\n",
    "        for index, data in df.iterrows():\n",
    "            if is_nan(data, target_column) and not is_nan(data. column):\n",
    "                df.loc[index][target_column] = data[column]\n",
    "    \n",
    "    new_index = df.columns\n",
    "    for column in column_list:\n",
    "        new_index = new_index.drop(column)\n",
    "        \n",
    "    df = df[new_index.tolist()]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확진일 관련 column `comfirmed_date`로 통일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merge_column(merged_data, 'confirmed_date', ['확진일', 'confiemd_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "노출일자 관련 column `date`로 통일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merge_column(merged_data, 'date', [' date', '노출일자'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "환자 일련번호 관련 column `patient_id`로 통일(`local_id` 제외)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merge_column(merged_data, 'patient_id', ['id', 'patent_id', 'patient', 'global_id', '확진자'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "감염경로 관련 column `reason`으로 통일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merge_column(merged_data, 'reason', ['감염경로'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "방문지 주소 관련 column `location_address`로 통일(`region` 제외)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merge_column(merged_data, 'location_address', [' location_address', 'exact_address', '주소', 'location'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "방문지 지역 관련 column `location_region`으로 통일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merge_column(merged_data, 'location_region', [' location_region'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "방문지 유형 관련 column `location_type`으로 통일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merge_column(merged_data, 'location_type', ['유형', 'place_type', 'type', 'location_name', ' location_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인적사항 관련 column `personal_info`로 통일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merge_column(merged_data, 'personal_info', ['인적사항', '개인정보'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "소독여부 관련 column `is_infected`로 통일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merge_column(merged_data, 'is_infected', ['disinfection', '소독 여부'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불필요한 columns 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['접촉', '격리시설', 'is_infected']\n",
    "\n",
    "new_index = merged_data.columns\n",
    "for column in columns_to_drop:\n",
    "    new_index = new_index.drop(column)\n",
    "    \n",
    "merged_data = merged_data[new_index.tolist()]\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "중간 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path = join(root, 'data', 'extracted', 'merged_route_check_01.csv')\n",
    "merged_data.to_csv(path, encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "중간 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = join(root, 'data', 'extracted', 'merged_route_check_01.csv')\n",
    "merged_data = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "date 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_into_format(str_date, previous_date_list):\n",
    "    if not isinstance(str_date, str): return []\n",
    "    \n",
    "    # 일반 날짜 형식 정규식\n",
    "    p_date1 = '(\\d+\\.)*\\d+\\.\\s*\\d+\\.*(\\([가-힣]\\))*\\s*' # 8.27(목) 형식\n",
    "    p_date2 = '\\d+\\.\\s*\\d+\\.\\d+\\.*(\\([가-힣]\\))*\\s*' # 20.8.27(목) 형식\n",
    "    p_date3 = '\\d+월\\s*\\d+일*\\s*(\\([가-힣]\\))*\\s*' # 1월 1일(수) 형식\n",
    "    p_date4 = '\\d+\\/\\d+(\\([가-힣]\\))*' # 1/1(수) 형식\n",
    "    p_date5 = '\\d+\\.*\\([가-힣]\\)'\n",
    "\n",
    "    pattern_list = [p_date1, p_date2, p_date3, p_date4]\n",
    "\n",
    "    # 중복 확인하기 위한 정규식\n",
    "    dup_date1 = '~\\s*(\\d+\\.)*\\d+\\.\\s*\\d+\\.*(\\([가-힣]\\))*' # ~1.1(월) 형식\n",
    "    dup_date2 = '~\\s*\\d+\\s일*(\\([가-힣]\\))*' # ~1일(월) 형식\n",
    "    dup_date3 = '~\\s*\\d+\\/\\d+(\\([가-힣]\\))*' # ~1/1(월) 형식\n",
    "    \n",
    "    dup_list = [dup_date1, dup_date2, dup_date3]\n",
    "    \n",
    "    # 중복 있는 경우\n",
    "    elements = []\n",
    "    for dup_pattern in dup_list:\n",
    "        end_date = re.search(r'%s' % dup_pattern, str_date)\n",
    "        if end_date is None: continue\n",
    "        \n",
    "        # 끝나는 날짜\n",
    "        end_date = end_date.group()\n",
    "        elements = [int(x.group()) for x in re.finditer(r'\\d+', end_date)]\n",
    "        \n",
    "        break\n",
    "\n",
    "    # 일반적인 경우\n",
    "    found_date_list = []\n",
    "    formed_date_list = []\n",
    "    for pattern in pattern_list:\n",
    "        found_date = re.search(r'%s' % pattern, str_date)\n",
    "        if found_date is None: continue\n",
    "        \n",
    "        found_date_list = [x.group() for x in re.finditer(r'%s' % pattern, str_date)]\n",
    "        for date in found_date_list:\n",
    "            date_elem_list = [int(x.group()) for x in re.finditer(r'\\d+', date)]\n",
    "            if len(date_elem_list) > 2 and date_elem_list[0] > 12:\n",
    "                del date_elem_list[0]\n",
    "            if date_elem_list[0] > 12: continue\n",
    "            formed_date = datetime.datetime(2020, date_elem_list[0], date_elem_list[1], 0, 0)\n",
    "            formed_date = formed_date.strftime('%Y-%m-%d')\n",
    "            formed_date_list.append(formed_date)\n",
    "            \n",
    "        break\n",
    "        \n",
    "    # 중복 있는 경우 마지막 날부터 중복 날까지 append\n",
    "    if len(elements) != 0:\n",
    "        if len(elements) == 1:\n",
    "            month = int(re.search(r'(?<=-).*(?=-)', formed_date_list[-1]).group())\n",
    "            day = elements[0]\n",
    "        elif len(elements) == 2:\n",
    "            month = elements[0]\n",
    "            day = elements[1]\n",
    "        else:\n",
    "            month = elements[1]\n",
    "            day = elements[2]\n",
    "        \n",
    "        end_date = datetime.datetime(2020, month, day, 0, 0)\n",
    "        normal_last_day = formed_date_list[-1]\n",
    "        normal_last_day = datetime.datetime.strptime(normal_last_day, '%Y-%m-%d')\n",
    "        delta = end_date - normal_last_day\n",
    "        \n",
    "        dates_to_append = []\n",
    "        for i in range(delta.days + 1):\n",
    "            day = normal_last_day + timedelta(days=i)\n",
    "            dates_to_append.append(day.strftime('%Y-%m-%d'))\n",
    "        \n",
    "        formed_date_list += dates_to_append\n",
    "        formed_date_list.append(normal_last_day)\n",
    "        \n",
    "    # formed_date_list가 비어 있는 경우. 이전 row의 날짜 받는지, 일만 표기되어 있는 것이 아닌지\n",
    "    if len(formed_date_list) == 0:\n",
    "        # 이전 row의 날짜 받는지\n",
    "        if re.search(r'^\\s*-\\s*$', str_date) is not None:\n",
    "            return previous_date_list\n",
    "        if re.search(r'%s' % p_date5, str_date) is not None:\n",
    "            day_list = [x.group() for x in re.finditer(r'%s' % p_date5, str_date)]\n",
    "            for i, day in enumerate(day_list):\n",
    "                day_list[i] = int(re.search(r'\\d+', day).group())\n",
    "                \n",
    "            if len(previous_date_list) == 0: return formed_date_list\n",
    "            \n",
    "            month = int(re.search(r'(?<=-).*(?=-)', previous_date_list[-1]).group())\n",
    "            for day in day_list:\n",
    "                new_date = datetime.datetime(2020, month, day, 0, 0)\n",
    "                new_date = new_date.strftime('%Y-%m-%d')\n",
    "                formed_date_list.append(new_date)\n",
    "        \n",
    "    return formed_date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "found_date_list = []\n",
    "new_row_list = []\n",
    "indices_to_del = []\n",
    "for index, data in merged_data.iterrows():\n",
    "    found_date_list = date_into_format(data['date'], found_date_list)\n",
    "    \n",
    "    if len(found_date_list) == 0:\n",
    "        continue\n",
    "    if len(found_date_list) == 1:\n",
    "        merged_data.loc[index, 'date'] = found_date_list[0]\n",
    "    if len(found_date_list) > 1:\n",
    "        for found_date in found_date_list:\n",
    "            new_row = merged_data.loc[index]\n",
    "            new_row['date'] = found_date\n",
    "            new_row_list.append(new_row)\n",
    "        \n",
    "        indices_to_del.append(index)\n",
    "        \n",
    "merged_data = merged_data.drop(indices_to_del)\n",
    "\n",
    "for new_row in new_row_list:\n",
    "    merged_data = merged_data.append(new_row, ignore_index=True)\n",
    "\n",
    "merged_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "중간 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = join(root, 'data', 'extracted', 'merged_route_check_02.csv')\n",
    "merged_data.to_csv(path, encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
