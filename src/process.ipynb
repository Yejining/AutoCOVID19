{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import h5py\n",
    "from keras.utils.io_utils import HDF5Matrix\n",
    "from datetime import datetime, timedelta\n",
    "from PIL import Image\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def unique_value(array, property):\n",
    "    array = array[property]\n",
    "    array = array.drop_duplicates(keep='last')\n",
    "    array = array.tolist()\n",
    "    array.sort()\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = 0\n",
    "max_path = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting general\n",
    "name = '../covid_dataset/9th_epoch200_relu/' \n",
    "size = 256\n",
    "kernel_size = 64\n",
    "weight = 5\n",
    "\n",
    "# setting path\n",
    "patient_daily_path = 'patient_daily/'\n",
    "patient_daily_3days_accumulated_path = 'patient_daily_3days_accumulated/'\n",
    "all_daily_path = 'all_daily/'\n",
    "figure_path = 'figure/'\n",
    "dataset_path = 'merged.h5'\n",
    "dataset_figure_path = 'figure.h5'\n",
    "model_path = 'model.h5'\n",
    "pred_path = 'pred/'\n",
    "x_test_path = 'x_test/'\n",
    "y_test_path = 'y_test/'\n",
    "y_pred_path = 'pred/'\n",
    "readme_path = 'README.md'\n",
    "scaled_path = 'scaled/'\n",
    "\n",
    "# getting raw dataset\n",
    "routes = pd.read_csv('../covid_dataset/raw/MergedRoute.csv')\n",
    "dates = unique_value(routes, 'date')\n",
    "patients = unique_value(routes, 'patient_id')\n",
    "\n",
    "# getting figure dataset\n",
    "figure_routes = pd.read_csv('../covid_dataset/raw/Figure.csv')\n",
    "figure_dates = unique_value(figure_routes, 'date')\n",
    "figure_patients = unique_value(figure_routes, 'patient_id')\n",
    "\n",
    "# dataset info\n",
    "first_day = datetime.strptime(dates[0], \"%Y-%m-%d\")\n",
    "last_day = datetime.strptime(dates[-1], \"%Y-%m-%d\") # + timedelta(days=3)\n",
    "delta = last_day - first_day\n",
    "duration = delta.days + 1\n",
    "\n",
    "# setting features\n",
    "names = ['age', 'sex', 'infection_case', 'type', 'date']\n",
    "counts = [11, 2, 4, 21, 7]\n",
    "visit_types = ['karaoke', 'gas_station', 'gym', 'bakery', 'pc_cafe',\n",
    "              'beauty_salon', 'school', 'church', 'bank', 'cafe',\n",
    "              'bar', 'post_office', 'real_estate_agency', 'lodging',\n",
    "              'public_transportation', 'restaurant', 'etc', 'store',\n",
    "              'hospital', 'pharmacy', 'airport']\n",
    "causes = ['community infection', 'etc', 'contact with patient', 'overseas inflow']\n",
    "\n",
    "# setting model\n",
    "channel = sum(count for count in counts)\n",
    "split_num = 0\n",
    "n_step = 3\n",
    "epochs = 200\n",
    "batch_size = 1\n",
    "optimizer = 'rmsprop' # 'adadelta'\n",
    "loss = 'mean_squared_error' # 'binary_crossentropy'\n",
    "activation = 'relu' # 'relu', 'sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patient:\n",
    "    def age_category(age):\n",
    "        age = int(age[:-1])\n",
    "        if age == 0: return 0\n",
    "        elif age == 100: return 10\n",
    "        return age // 10\n",
    "            \n",
    "    def sex_category(sex):\n",
    "        if sex == 'male': return 0\n",
    "        return 1\n",
    "\n",
    "    def infection_case_category(infection_case, causes):\n",
    "        return causes.index(infection_case)\n",
    "    \n",
    "    def type_category(visit_type, move_types):\n",
    "        return move_types.index(visit_type)\n",
    "\n",
    "    def day_category(day):\n",
    "        day = datetime.strptime(day, \"%Y-%m-%d\")\n",
    "        return day.weekday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_places(places, counts, causes, visit_types):\n",
    "    indices = []\n",
    "    for i in range(len(places)):\n",
    "        one_visit = places.iloc[i]\n",
    "        indices.append(df_to_grid_index(one_visit, counts, causes, visit_types))\n",
    "    return indices\n",
    "\n",
    "def df_to_grid_index(one_visit, counts, causes, visit_types):\n",
    "    index = 0\n",
    "    p_age = Patient.age_category(one_visit['age'])\n",
    "    index += counts[0]\n",
    "    p_sex = Patient.sex_category(one_visit['sex']) + index\n",
    "    index += counts[1]\n",
    "    p_infection_case = Patient.infection_case_category(one_visit['infection_case'], causes) + index\n",
    "    index += counts[2]\n",
    "    p_type = Patient.type_category(one_visit['type'], visit_types) + index\n",
    "    index += counts[3]\n",
    "    p_date = Patient.day_category(one_visit['date']) + index\n",
    "    row = one_visit['row']\n",
    "    col = one_visit['col']\n",
    "    \n",
    "    return [p_age, p_sex, p_infection_case, p_type, p_date, row, col]\n",
    "\n",
    "def put_triangular_kernel(array, row, col, value):\n",
    "    stride = int((kernel_size - 1) / 2)\n",
    "    ratio = 1 / (stride + 1)\n",
    "   \n",
    "    for i in range(row - stride, row + stride + 1):\n",
    "        if i < 0 or i >= array.shape[0]: continue\n",
    "        for j in range(col - stride, col + stride + 1):\n",
    "            if j < 0 or j >= array.shape[1]: continue\n",
    "            distance = math.sqrt((row - i)**2 + (col - j)**2)\n",
    "            new_value = value * (1 - (distance * ratio))\n",
    "            if new_value < 0: new_value = 0\n",
    "            array[i][j] = new_value\n",
    "       \n",
    "    array[row][col] = value\n",
    "    return array\n",
    "\n",
    "def overlay_kernel(array):\n",
    "    new_image = np.zeros((array.shape[0],array.shape[1]))\n",
    "    for row in range(array.shape[0]):\n",
    "        for col in range(array.shape[1]):\n",
    "            if array[row][col] == 0: continue\n",
    "            new_image += put_triangular_kernel(np.zeros((array.shape[0], array.shape[1])), row, col, array[row][col])\n",
    "\n",
    "    return new_image\n",
    "\n",
    "def indices_save_image(path, place_indices):\n",
    "    all_counts = sum(count for count in counts)\n",
    "    visit_grid = np.zeros((all_counts, size, size))\n",
    "    \n",
    "    for index in place_indices:\n",
    "        row = index[5]\n",
    "        col = index[6]\n",
    "        for feature in range(5):\n",
    "            visit_grid[index[feature]][row][col] += weight\n",
    "    \n",
    "    for channel in range(visit_grid.shape[0]):\n",
    "        save_grid(path + str(channel) + \".png\", visit_grid[channel])\n",
    "    \n",
    "def save_grid(path, grid, kernel=True):\n",
    "    global max_value, max_path\n",
    "    if kernel: grid = overlay_kernel(grid)\n",
    "    if np.amax(grid) > max_value:\n",
    "        max_value = np.amax(grid)\n",
    "        max_path = path\n",
    "    img = Image.fromarray(grid.astype('uint8'), 'L')\n",
    "    img.save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_patient_route(path, patient, routes):\n",
    "    patient_places = routes[routes['patient_id']==patient]\n",
    "    patient_dates = unique_value(patient_places, 'date')\n",
    "    f_first_day = datetime.strptime(patient_dates[0], \"%Y-%m-%d\")\n",
    "    f_last_day = datetime.strptime(patient_dates[-1], \"%Y-%m-%d\") + timedelta(days=3)\n",
    "    f_delta = f_last_day - f_first_day\n",
    "    f_duration = f_delta.days + 1\n",
    "\n",
    "    # 저장\n",
    "    patient_path = path + str(patient)\n",
    "    Path(patient_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    today = f_first_day\n",
    "    while(True):\n",
    "        today_str = datetime.strftime(today, \"%Y-%m-%d\")\n",
    "        patient_day_places = patient_places[patient_places['date']==today_str]\n",
    "        places_indices = combine_places(patient_day_places, counts, causes, visit_types)\n",
    "        patient_date_path = patient_path + \"/\" + today_str + '/'\n",
    "        Path(patient_date_path).mkdir(parents=True, exist_ok=True)\n",
    "        indices_save_image(patient_date_path, places_indices)\n",
    "        if today == f_last_day: break\n",
    "        today += timedelta(days=1)\n",
    "\n",
    "def get_patient_route(patient, routes):\n",
    "    patient_places = routes[routes['patient_id']==patient]\n",
    "    patient_dates = unique_value(patient_places, 'date')\n",
    "    f_first_day = datetime.strptime(patient_dates[0], \"%Y-%m-%d\")\n",
    "    f_last_day = datetime.strptime(patient_dates[-1], \"%Y-%m-%d\") + timedelta(days=3)\n",
    "    f_delta = f_last_day - f_first_day\n",
    "    f_duration = f_delta.days + 1\n",
    "\n",
    "    patient_routes = []\n",
    "    today = f_first_day\n",
    "    while(True):\n",
    "        today_str = datetime.strftime(today, \"%Y-%m-%d\")\n",
    "        patient_day_places = patient_places[patient_places['date']==today_str]\n",
    "        places_indices = combine_places(patient_day_places, counts, causes, visit_types)\n",
    "        patient_routes.append([today_str, places_indices]) \n",
    "\n",
    "        if today == f_last_day: break\n",
    "        today += timedelta(days=1)\n",
    "        \n",
    "    return patient_routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### raw data 저장\n",
    "\n",
    "#### 확진자 기준으로 raw data 저장\n",
    "```\n",
    "path = name + patient_daily_path\n",
    "for patient in patients:\n",
    "    save_patient_route(path, patient, routes)\n",
    "```\n",
    "\n",
    "#### figure 저장\n",
    "```\n",
    "path = name + figure_path + patient_daily_path\n",
    "for patient in figure_patients:\n",
    "    save_patient_route(path, patient, figure_routes)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3일 누적 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_two_days(day1, day2):\n",
    "    day2[1].extend(day1[1])\n",
    "    return day2\n",
    "    \n",
    "def accumulate_patient(patient, routes):\n",
    "    patient_route = get_patient_route(patient, routes)\n",
    "    patient_days = len(patient_route)\n",
    "\n",
    "    second = patient_days - 1\n",
    "    first = second - 1\n",
    "    for i in range(2 * patient_days - 3):\n",
    "        patient_route[second] = accumulate_two_days(patient_route[first], patient_route[second])\n",
    "        if second - first == 2: second -=1\n",
    "        else: first -= 1\n",
    "    \n",
    "    return patient_route\n",
    "\n",
    "def save_patient_routes(path, patient, patient_routes):\n",
    "    path += str(patient) + '/'\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    for routes in patient_routes:\n",
    "        patient_date_path = path + routes[0] + '/'\n",
    "        Path(patient_date_path).mkdir(parents=True, exist_ok=True)\n",
    "        indices_save_image(patient_date_path, routes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 누적 경로 저장\n",
    "\n",
    "#### 확진자 기준으로 저장\n",
    "```\n",
    "path = name + patient_daily_3days_accumulated_path\n",
    "for patient in patients:\n",
    "    accumulated_routes = accumulate_patient(patient, routes)\n",
    "    save_patient_routes(path, patient, accumulated_routes)\n",
    "```\n",
    "\n",
    "#### figure 저장    \n",
    "```\n",
    "path = name + figure_path + patient_daily_3days_accumulated_path\n",
    "for patient in figure_patients:\n",
    "    accumulated_routes = accumulate_patient(patient, figure_routes)\n",
    "    save_patient_routes(path, patient, accumulated_routes)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 날짜별 취합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complete_routes(routes, dates, patients):\n",
    "    f_first_day = datetime.strptime(dates[0], \"%Y-%m-%d\")\n",
    "    f_last_day = datetime.strptime(dates[-1], \"%Y-%m-%d\") + timedelta(days=3)\n",
    "\n",
    "    # 날짜별 경로 배열 생성\n",
    "    today = f_first_day\n",
    "    complete_routes = []\n",
    "    while(True):\n",
    "        today_str = datetime.strftime(today, \"%Y-%m-%d\")\n",
    "        places = []\n",
    "        complete_routes.append([today_str, places])\n",
    "\n",
    "        if today == f_last_day: break\n",
    "        today += timedelta(days=1)\n",
    "\n",
    "    # 환자 경로 가져온 다음 날짜대로 배치\n",
    "    for patient in patients:\n",
    "        accumulated_routes = accumulate_patient(patient, routes)\n",
    "        for each_route in accumulated_routes:\n",
    "            route_day = datetime.strptime(each_route[0], \"%Y-%m-%d\")\n",
    "            route_places = each_route[1]\n",
    "            index = (route_day - f_first_day).days\n",
    "            complete_routes[index][1].extend(route_places)\n",
    "    return complete_routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 취합한 경로 저장\n",
    "\n",
    "#### 확진자 기준으로 저장\n",
    "```\n",
    "path = name + all_daily_path\n",
    "complete_routes = get_complete_routes(routes, dates, patients)\n",
    "for days in complete_routes:\n",
    "    date_path = path + days[0] + '/'\n",
    "    Path(date_path).mkdir(parents=True, exist_ok=True)\n",
    "    indices_save_image(date_path, days[1])\n",
    "```\n",
    "\n",
    "#### figure 저장\n",
    "```\n",
    "path = name + figure_path + all_daily_path\n",
    "complete_routes = get_complete_routes(figure_routes, figure_dates, figure_patients)\n",
    "for days in complete_routes:\n",
    "    date_path = path + days[0] + '/'\n",
    "    Path(date_path).mkdir(parents=True, exist_ok=True)\n",
    "    indices_save_image(date_path, days[1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(routes, dates, patients):\n",
    "    complete_routes = get_complete_routes(routes, dates, patients)\n",
    "    all_counts = sum(count for count in counts)\n",
    "    dataset = np.zeros((len(complete_routes), all_counts, size, size))\n",
    "\n",
    "    for i, days in enumerate(complete_routes):\n",
    "        sub_routes = get_array_image(days, dataset[i,:,:,:])\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def get_array_image(place_indices, data_array):\n",
    "    all_counts = sum(count for count in counts)\n",
    "    \n",
    "    for index in place_indices[1]:\n",
    "        row = index[5]\n",
    "        col = index[6]\n",
    "        for feature in range(5):\n",
    "            data_array[index[feature]][row][col] += weight\n",
    "    \n",
    "    for channel in range(data_array.shape[0]):\n",
    "        data_array[channel] = overlay_kernel(data_array[channel])\n",
    "    \n",
    "    return data_array\n",
    "    \n",
    "def divide_dataset(dataset):\n",
    "    n = dataset.shape[0]\n",
    "    channel = dataset.shape[1]\n",
    "\n",
    "    X_set = []\n",
    "    y_set = []\n",
    "    for i in range(n_step, n):\n",
    "        X_set.append(dataset[i-n_step:i, :,:])\n",
    "        y_set.append(dataset[i:i+1, :,:])\n",
    "    X_set, y_set = np.array(X_set), np.array(y_set)\n",
    "   \n",
    "    return X_set, y_set\n",
    "    \n",
    "def save_dataset(path, X_set, y_set, start_day):\n",
    "    start_day = start_day.split('-')\n",
    "    start_day = list(map(int, start_day))\n",
    "    \n",
    "    with h5py.File(path,'w') as f:    \n",
    "        set_X = f.create_dataset('X_set', data=X_set)\n",
    "        set_y = f.create_dataset('y_set', data=y_set)\n",
    "        set_day = f.create_dataset('start_day', data=np.array(start_day))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save dataset\n",
    "\n",
    "#### 확진자 기준으로 저장\n",
    "```\n",
    "path = name + dataset_path\n",
    "dataset = get_dataset(routes, dates, patients)\n",
    "X_set, y_set = divide_dataset(dataset)\n",
    "save_dataset(path, X_set, y_set, dates[0])\n",
    "print(X_set.shape, y_set.shape)\n",
    "```\n",
    "\n",
    "#### figure 저장\n",
    "```\n",
    "path = name + dataset_figure_path\n",
    "dataset = get_dataset(figure_routes, figure_dates, figure_patients)\n",
    "X_set, y_set = divide_dataset(dataset)\n",
    "save_dataset(path, X_set, y_set, figure_dates[0])\n",
    "print(X_set.shape, y_set.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.convolutional import Conv3D\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras.backend.tensorflow_backend as K\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as tfback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a layer which take as input movies of shape\n",
    "# (n_frames, width, height, channels) and returns a movie\n",
    "# of identical shape.\n",
    "def get_model(channel):\n",
    "    with K.tf_ops.device('/GPU:0'):\n",
    "        seq = Sequential()\n",
    "        seq.add(ConvLSTM2D(filters=channel, kernel_size=(3, 3), data_format='channels_first',\n",
    "                           input_shape=(n_step, channel, size, size),\n",
    "                           padding='same', return_sequences=True))\n",
    "        seq.add(BatchNormalization())\n",
    "\n",
    "        seq.add(ConvLSTM2D(filters=channel, kernel_size=(3, 3), data_format='channels_first',\n",
    "                           padding='same', return_sequences=True))\n",
    "        seq.add(BatchNormalization())\n",
    "\n",
    "        seq.add(ConvLSTM2D(filters=channel, kernel_size=(3, 3), data_format='channels_first',\n",
    "                           padding='same', return_sequences=True))\n",
    "        seq.add(BatchNormalization())\n",
    "\n",
    "        seq.add(ConvLSTM2D(filters=channel, kernel_size=(3, 3), data_format='channels_first',\n",
    "                           padding='same', return_sequences=True))\n",
    "        seq.add(BatchNormalization())\n",
    "\n",
    "        seq.add(Conv3D(filters=1, kernel_size=(3, 3, 3),\n",
    "                       activation=activation,\n",
    "                       padding='same', data_format='channels_first'))\n",
    "        \n",
    "        seq.compile(optimizer=optimizer, loss=loss)\n",
    "                \n",
    "        seq.summary()\n",
    "\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, n_train):\n",
    "    X_train = HDF5Matrix(path, 'X_set', start=0, end=n_train)\n",
    "    y_train = HDF5Matrix(path, 'y_set', start=0, end=n_train)\n",
    "    X_test = HDF5Matrix(path, 'X_set', start=n_train)\n",
    "    y_test = HDF5Matrix(path, 'y_set', start=n_train)\n",
    "    \n",
    "    start_day = np.array(HDF5Matrix(path, 'start_day'))\n",
    "    start_day = \"%d-%.2d-%.2d\" % (start_day[0], start_day[1], start_day[2])\n",
    "    start_day1 = datetime.strptime(start_day, \"%Y-%m-%d\")\n",
    "    start_day2 = start_day1 + timedelta(days=n_train)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, start_day1, start_day2\n",
    "\n",
    "def figure_load_data(path):\n",
    "    X_test = HDF5Matrix(path, 'X_set')\n",
    "    y_test = HDF5Matrix(path, 'y_set')\n",
    "    start_day = np.array(HDF5Matrix(path, 'start_day'))\n",
    "    start_day = \"%d-%.2d-%.2d\" % (start_day[0], start_day[1], start_day[2])\n",
    "    start_day = datetime.strptime(start_day, \"%Y-%m-%d\")\n",
    "    \n",
    "    return X_test, y_test, start_day\n",
    "\n",
    "def get_array_image(place_indices, data_array):\n",
    "    all_counts = sum(count for count in counts)\n",
    "    \n",
    "    for index in place_indices[1]:\n",
    "        row = index[5]\n",
    "        col = index[6]\n",
    "        for feature in range(5):\n",
    "            data_array[index[feature]][row][col] += weight\n",
    "    \n",
    "    for channel in range(data_array.shape[0]):\n",
    "        data_array[channel] = overlay_kernel(data_array[channel])\n",
    "    \n",
    "    return data_array\n",
    "\n",
    "def train(path, X_train, y_train):\n",
    "    seq = get_model(channel)\n",
    "    seq.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, shuffle='batch')\n",
    "    seq.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vislab/.local/lib/python3.7/site-packages/keras/utils/io_utils.py:60: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  f = h5py.File(datapath)\n"
     ]
    }
   ],
   "source": [
    "#### 데이터셋 불러오기\n",
    "\n",
    "#### 확진자\n",
    "\n",
    "path = name + dataset_path\n",
    "sample_num = duration - (2 * n_step)\n",
    "if split_num == 0: split_num = int(sample_num * 0.7)\n",
    "X_train, y_train, X_test, y_test, start_day1, start_day2 = load_data(path, split_num)\n",
    "\n",
    "#### figure\n",
    "\n",
    "path = name + dataset_figure_path\n",
    "X_figure, y_figure, start_figure = figure_load_data(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_lst_m2d_1 (ConvLSTM2D)  (None, 3, 45, 256, 256)   145980    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 3, 45, 256, 256)   1024      \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_2 (ConvLSTM2D)  (None, 3, 45, 256, 256)   145980    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 3, 45, 256, 256)   1024      \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_3 (ConvLSTM2D)  (None, 3, 45, 256, 256)   145980    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 3, 45, 256, 256)   1024      \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_4 (ConvLSTM2D)  (None, 3, 45, 256, 256)   145980    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 3, 45, 256, 256)   1024      \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 1, 45, 256, 256)   82        \n",
      "=================================================================\n",
      "Total params: 588,098\n",
      "Trainable params: 586,050\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "63/63 [==============================] - 33s 517ms/step - loss: 13.4838\n",
      "Epoch 2/200\n",
      "63/63 [==============================] - 18s 293ms/step - loss: 11.3132\n",
      "Epoch 3/200\n",
      "63/63 [==============================] - 18s 292ms/step - loss: 9.9854\n",
      "Epoch 4/200\n",
      "63/63 [==============================] - 18s 293ms/step - loss: 9.0804\n",
      "Epoch 5/200\n",
      "63/63 [==============================] - 19s 294ms/step - loss: 8.4045\n",
      "Epoch 6/200\n",
      "63/63 [==============================] - 19s 294ms/step - loss: 7.8044\n",
      "Epoch 7/200\n",
      "63/63 [==============================] - 19s 294ms/step - loss: 7.2480\n",
      "Epoch 8/200\n",
      "63/63 [==============================] - 19s 294ms/step - loss: 6.9516\n",
      "Epoch 9/200\n",
      "63/63 [==============================] - 19s 295ms/step - loss: 6.2286\n",
      "Epoch 10/200\n",
      "63/63 [==============================] - 19s 295ms/step - loss: 6.2440\n",
      "Epoch 11/200\n",
      "63/63 [==============================] - 19s 296ms/step - loss: 5.4604\n",
      "Epoch 12/200\n",
      "63/63 [==============================] - 19s 295ms/step - loss: 5.7127\n",
      "Epoch 13/200\n",
      "63/63 [==============================] - 19s 296ms/step - loss: 5.1604\n",
      "Epoch 14/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 4.9855\n",
      "Epoch 15/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 4.5189\n",
      "Epoch 16/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 4.4112\n",
      "Epoch 17/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 4.1958\n",
      "Epoch 18/200\n",
      "63/63 [==============================] - 19s 296ms/step - loss: 4.3093\n",
      "Epoch 19/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 3.8565\n",
      "Epoch 20/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 3.8381\n",
      "Epoch 21/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 3.6833\n",
      "Epoch 22/200\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 3.7592\n",
      "Epoch 23/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 3.3716\n",
      "Epoch 24/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 3.3722\n",
      "Epoch 25/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 3.3174\n",
      "Epoch 26/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 3.3616\n",
      "Epoch 27/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 3.1590\n",
      "Epoch 28/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 3.0308\n",
      "Epoch 29/200\n",
      "63/63 [==============================] - 19s 301ms/step - loss: 3.0564\n",
      "Epoch 30/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 2.9321\n",
      "Epoch 31/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 3.1487\n",
      "Epoch 32/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 2.7807\n",
      "Epoch 33/200\n",
      "63/63 [==============================] - 19s 296ms/step - loss: 2.9487\n",
      "Epoch 34/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 2.6235\n",
      "Epoch 35/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 2.6951\n",
      "Epoch 36/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 2.5677\n",
      "Epoch 37/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 2.6343\n",
      "Epoch 38/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 2.6005\n",
      "Epoch 39/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 2.4504\n",
      "Epoch 40/200\n",
      "63/63 [==============================] - 19s 301ms/step - loss: 2.3673\n",
      "Epoch 41/200\n",
      "63/63 [==============================] - 19s 296ms/step - loss: 2.3974\n",
      "Epoch 42/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 2.3877\n",
      "Epoch 43/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 2.3726\n",
      "Epoch 44/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 2.2679\n",
      "Epoch 45/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 2.3033\n",
      "Epoch 46/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 2.1539\n",
      "Epoch 47/200\n",
      "63/63 [==============================] - 19s 301ms/step - loss: 2.2728\n",
      "Epoch 48/200\n",
      "63/63 [==============================] - 19s 306ms/step - loss: 2.1094\n",
      "Epoch 49/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 2.0828\n",
      "Epoch 50/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 2.1275\n",
      "Epoch 51/200\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 2.1082\n",
      "Epoch 52/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 2.1524\n",
      "Epoch 53/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.9249\n",
      "Epoch 54/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.9848\n",
      "Epoch 55/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.9068\n",
      "Epoch 56/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 1.9183\n",
      "Epoch 57/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 1.8740\n",
      "Epoch 58/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 1.9435\n",
      "Epoch 59/200\n",
      "63/63 [==============================] - 19s 296ms/step - loss: 1.8917\n",
      "Epoch 60/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 1.9033\n",
      "Epoch 61/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 1.8613\n",
      "Epoch 62/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 1.7866\n",
      "Epoch 63/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.7715\n",
      "Epoch 64/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 1.7099\n",
      "Epoch 65/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 1.8294\n",
      "Epoch 66/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.7089\n",
      "Epoch 67/200\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 1.7218\n",
      "Epoch 68/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.6740\n",
      "Epoch 69/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 1.6452\n",
      "Epoch 70/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 1.6381\n",
      "Epoch 71/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 1.5890\n",
      "Epoch 72/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 1.7746\n",
      "Epoch 73/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.5635\n",
      "Epoch 74/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.4957\n",
      "Epoch 75/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.5874\n",
      "Epoch 76/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 1.6244\n",
      "Epoch 77/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.5813\n",
      "Epoch 78/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.4784\n",
      "Epoch 79/200\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 1.5805\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 19s 300ms/step - loss: 1.5043\n",
      "Epoch 81/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.4410\n",
      "Epoch 82/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.5089\n",
      "Epoch 83/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.4864\n",
      "Epoch 84/200\n",
      "63/63 [==============================] - 19s 296ms/step - loss: 1.4381\n",
      "Epoch 85/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 1.4850\n",
      "Epoch 86/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.3778\n",
      "Epoch 87/200\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 1.5251\n",
      "Epoch 88/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 1.4127\n",
      "Epoch 89/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 1.3685\n",
      "Epoch 90/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.4168\n",
      "Epoch 91/200\n",
      "63/63 [==============================] - 19s 301ms/step - loss: 1.3293\n",
      "Epoch 92/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 1.3775\n",
      "Epoch 93/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 1.4418\n",
      "Epoch 94/200\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 1.3283\n",
      "Epoch 95/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 1.3996\n",
      "Epoch 96/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 1.3120\n",
      "Epoch 97/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 1.2850\n",
      "Epoch 98/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 1.3425\n",
      "Epoch 99/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.3456\n",
      "Epoch 100/200\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 1.2484\n",
      "Epoch 101/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 1.2549\n",
      "Epoch 102/200\n",
      "63/63 [==============================] - 19s 301ms/step - loss: 1.2848\n",
      "Epoch 103/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.2980\n",
      "Epoch 104/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.3450\n",
      "Epoch 105/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 1.2907\n",
      "Epoch 106/200\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 1.2629\n",
      "Epoch 107/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 1.2284\n",
      "Epoch 108/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 1.1646\n",
      "Epoch 109/200\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 1.2211\n",
      "Epoch 110/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.2266\n",
      "Epoch 111/200\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 1.2544\n",
      "Epoch 112/200\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 1.2156\n",
      "Epoch 113/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.1811\n",
      "Epoch 114/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.2171\n",
      "Epoch 115/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.2032\n",
      "Epoch 116/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 1.2147\n",
      "Epoch 117/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 1.1513\n",
      "Epoch 118/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 1.1724\n",
      "Epoch 119/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.1647\n",
      "Epoch 120/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.1850\n",
      "Epoch 121/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 1.1456\n",
      "Epoch 122/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 1.1574\n",
      "Epoch 123/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 1.1333\n",
      "Epoch 124/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.1427\n",
      "Epoch 125/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.1114\n",
      "Epoch 126/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.1056\n",
      "Epoch 127/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.1171\n",
      "Epoch 128/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.0795\n",
      "Epoch 129/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.0637\n",
      "Epoch 130/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 1.1327\n",
      "Epoch 131/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 1.1536\n",
      "Epoch 132/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.0666\n",
      "Epoch 133/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.0595\n",
      "Epoch 134/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.0618\n",
      "Epoch 135/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.1176\n",
      "Epoch 136/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.0141\n",
      "Epoch 137/200\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 1.0996\n",
      "Epoch 138/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.0111\n",
      "Epoch 139/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 1.0749\n",
      "Epoch 140/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 1.0472\n",
      "Epoch 141/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.9701\n",
      "Epoch 142/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 1.0594\n",
      "Epoch 143/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.9942\n",
      "Epoch 144/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 1.0164\n",
      "Epoch 145/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 1.0223\n",
      "Epoch 146/200\n",
      "63/63 [==============================] - 19s 296ms/step - loss: 0.9981\n",
      "Epoch 147/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.9416\n",
      "Epoch 148/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.0740\n",
      "Epoch 149/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.0081\n",
      "Epoch 150/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.0092\n",
      "Epoch 151/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.9852\n",
      "Epoch 152/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.0327\n",
      "Epoch 153/200\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 0.9928\n",
      "Epoch 154/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.9903\n",
      "Epoch 155/200\n",
      "63/63 [==============================] - 19s 296ms/step - loss: 0.9982\n",
      "Epoch 156/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 0.9680\n",
      "Epoch 157/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 0.9511\n",
      "Epoch 158/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 1.0279\n",
      "Epoch 159/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.9263\n",
      "Epoch 160/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 1.0021\n",
      "Epoch 161/200\n",
      "63/63 [==============================] - 19s 296ms/step - loss: 0.9594\n",
      "Epoch 162/200\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 1.0033\n",
      "Epoch 163/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.9436\n",
      "Epoch 164/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.9294\n",
      "Epoch 165/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 0.9824\n",
      "Epoch 166/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 0.9269\n",
      "Epoch 167/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 0.9167\n",
      "Epoch 168/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 0.9267\n",
      "Epoch 169/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 0.9391\n",
      "Epoch 170/200\n",
      "63/63 [==============================] - 19s 301ms/step - loss: 0.9645\n",
      "Epoch 171/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.9365\n",
      "Epoch 172/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.9581\n",
      "Epoch 173/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 0.9008\n",
      "Epoch 174/200\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 0.9002\n",
      "Epoch 175/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 0.9375\n",
      "Epoch 176/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 19s 299ms/step - loss: 0.9096\n",
      "Epoch 177/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 0.9485\n",
      "Epoch 178/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.8698\n",
      "Epoch 179/200\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 0.9022\n",
      "Epoch 180/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.8721\n",
      "Epoch 181/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.8658\n",
      "Epoch 182/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 0.8929\n",
      "Epoch 183/200\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 0.9129\n",
      "Epoch 184/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 0.8442\n",
      "Epoch 185/200\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 0.8562\n",
      "Epoch 186/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 0.9273\n",
      "Epoch 187/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 0.8504\n",
      "Epoch 188/200\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 0.8934\n",
      "Epoch 189/200\n",
      "63/63 [==============================] - 19s 302ms/step - loss: 0.8332\n",
      "Epoch 190/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 0.8995\n",
      "Epoch 191/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.9372\n",
      "Epoch 192/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 0.8230\n",
      "Epoch 193/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 0.8639\n",
      "Epoch 194/200\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 0.8488\n",
      "Epoch 195/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.8853\n",
      "Epoch 196/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.8655\n",
      "Epoch 197/200\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 0.8319\n",
      "Epoch 198/200\n",
      "63/63 [==============================] - 19s 301ms/step - loss: 0.8513\n",
      "Epoch 199/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.8442\n",
      "Epoch 200/200\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.8236\n"
     ]
    }
   ],
   "source": [
    "#### 트레이닝 함수 실행\n",
    "\n",
    "path = name + model_path\n",
    "train(path, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 1, 45, 256, 256)\n",
      "(11, 1, 45, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "#### predict 함수 실행\n",
    "\n",
    "#### 확진자\n",
    "\n",
    "path = name + model_path\n",
    "model = load_model(path)\n",
    "pred = model.predict(X_test)\n",
    "print(pred.shape)\n",
    "\n",
    "#### figure\n",
    "\n",
    "pred_figure = model.predict(X_figure)\n",
    "print(pred_figure.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prediction_image(path, X_test, y_test, pred, first_day):\n",
    "    # X_test\n",
    "    l_first_day = first_day\n",
    "    for l_sample in range(X_test.shape[0]):\n",
    "        sample_path = path + x_test_path + 'sample%d/' % l_sample\n",
    "        l_first_day2 = l_first_day\n",
    "        for l_day in range(X_test.shape[1]):\n",
    "            day_path = sample_path + datetime.strftime(l_first_day2, \"%Y-%m-%d\") + '/'\n",
    "            Path(day_path).mkdir(parents=True, exist_ok=True)\n",
    "            array_save_image(day_path, X_test[l_sample][l_day])\n",
    "            l_first_day2 += timedelta(days=1)\n",
    "        l_first_day += timedelta(days=1)\n",
    "\n",
    "    # y_test\n",
    "    l_first_day = first_day + timedelta(days=3)\n",
    "    for l_sample in range(y_test.shape[0]):\n",
    "        sample_path = path + y_test_path + datetime.strftime(l_first_day, \"%Y-%m-%d\") + '/'\n",
    "        Path(sample_path).mkdir(parents=True, exist_ok=True)\n",
    "        array_save_image(sample_path, y_test[l_sample][0])\n",
    "        l_first_day += timedelta(days=1)\n",
    "        \n",
    "    # pred\n",
    "    l_first_day = first_day + timedelta(days=3)\n",
    "    for l_sample in range(pred.shape[0]):\n",
    "        sample_path = path + y_pred_path + datetime.strftime(l_first_day, \"%Y-%m-%d\") + '/'\n",
    "        Path(sample_path).mkdir(parents=True, exist_ok=True)\n",
    "        array_save_image(sample_path, pred[l_sample][0])\n",
    "        \n",
    "        sample_path = path + scaled_path + datetime.strftime(l_first_day, \"%Y-%m-%d\") + '/'\n",
    "        Path(sample_path).mkdir(parents=True, exist_ok=True)\n",
    "        array_save_image(sample_path, pred[l_sample][0], scaled=True)\n",
    "        l_first_day += timedelta(days=1)     \n",
    "        \n",
    "def array_save_image(path, array, scaled=False):\n",
    "    for channel in range(array.shape[0]):\n",
    "        if scaled:\n",
    "            new_array = array[channel]\n",
    "            new_array[new_array >= 0] *= 255\n",
    "            new_array = np.asarray(new_array)\n",
    "            save_grid(path + str(channel) + \".png\", new_array, kernel=False)\n",
    "        else:\n",
    "            save_grid(path + str(channel) + \".png\", array[channel], kernel=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### save results\n",
    "\n",
    "#### 확진자\n",
    "\n",
    "path = name + pred_path\n",
    "save_prediction_image(path, X_test, y_test, pred, start_day2)\n",
    "\n",
    "### figure\n",
    "\n",
    "path = name + pred_path + figure_path\n",
    "save_prediction_image(path, X_figure, y_figure, pred_figure, start_figure)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "read = ['- dataset info',\n",
    "        '  - start_date: ',\n",
    "        '  - end_date: ',\n",
    "        '  - duration: ',\n",
    "        '- setting data preprocessing',\n",
    "        '  - image_size: ',\n",
    "        '  - kde_bandwidth: ',\n",
    "        '  - weight_per_person: ',\n",
    "        '- setting feature',\n",
    "        '  - feature_names: ',\n",
    "        '  - feature_counts: ',\n",
    "        '  - visit_types: ',\n",
    "        '  - causes: ',\n",
    "        '- setting model',\n",
    "        '  - epochs: ',\n",
    "        '  - batch_size: ',\n",
    "        '  - optimizer: ',\n",
    "        '  - loss: ', \n",
    "        '  - activation: ',\n",
    "        '  - #train, #test: ',\n",
    "        'extended 3 days of figure set']\n",
    "\n",
    "read[1] += datetime.strftime(first_day, \"%Y-%m-%d\")\n",
    "read[2] += datetime.strftime(last_day, \"%Y-%m-%d\")\n",
    "read[3] += str(duration)\n",
    "\n",
    "read[5] += str(size)\n",
    "read[6] += str(kernel_size)\n",
    "read[7] += str(weight)\n",
    "\n",
    "counts = map(str, counts)\n",
    "read[8] += ', '.join(names)\n",
    "read[10] += ', '.join(counts)\n",
    "read[11] += ', '.join(visit_types)\n",
    "read[12] += ', '.join(causes)\n",
    "\n",
    "sample_num = duration - (2 * n_step)\n",
    "train_num = int(sample_num * 0.7)\n",
    "test_num = sample_num - train_num\n",
    "read[14] += str(epochs)\n",
    "read[15] += str(batch_size)\n",
    "read[16] += optimizer\n",
    "read[17] += loss\n",
    "read[18] += activation\n",
    "read[18] += str(train_num) + \", \" + str(test_num)\n",
    "\n",
    "read = '\\n'.join(read)\n",
    "\n",
    "with open(name + readme_path, 'w') as f:\n",
    "    f.write(read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
