{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import h5py\n",
    "from keras.utils.io_utils import HDF5Matrix\n",
    "from datetime import datetime, timedelta\n",
    "from PIL import Image\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 256\n",
    "kernel_size = 64\n",
    "weight = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_value(array, property):\n",
    "    array = array[property]\n",
    "    array = array.drop_duplicates(keep='last')\n",
    "    array = array.tolist()\n",
    "    array.sort()\n",
    "    return array\n",
    "\n",
    "routes = pd.read_csv('covid19/MergedRoute.csv')\n",
    "dates = unique_value(routes, 'date')\n",
    "patients = unique_value(routes, 'patient_id')\n",
    "dataset_path = 'test_3.h5'\n",
    "model_path = 'model/0518_train_1.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['age', 'sex', 'infection_case', 'type', 'date']\n",
    "counts = [11, 2, 4, 21, 7]\n",
    "visit_types = ['karaoke', 'gas_station', 'gym', 'bakery', 'pc_cafe',\n",
    "              'beauty_salon', 'school', 'church', 'bank', 'cafe',\n",
    "              'bar', 'post_office', 'real_estate_agency', 'lodging',\n",
    "              'public_transportation', 'restaurant', 'etc', 'store',\n",
    "              'hospital', 'pharmacy', 'airport']\n",
    "causes = ['community infection', 'etc', 'contact with patient', 'overseas inflow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patient:\n",
    "    def age_category(age):\n",
    "        age = int(age[:-1])\n",
    "        if age == 0: return 0\n",
    "        elif age == 100: return 10\n",
    "        return age // 10\n",
    "            \n",
    "    def sex_category(sex):\n",
    "        if sex == 'male': return 0\n",
    "        return 1\n",
    "\n",
    "    def infection_case_category(infection_case, causes):\n",
    "        return causes.index(infection_case)\n",
    "    \n",
    "    def type_category(visit_type, move_types):\n",
    "        return move_types.index(visit_type)\n",
    "\n",
    "    def day_category(day):\n",
    "        day = datetime.strptime(day, \"%Y-%m-%d\")\n",
    "        return day.weekday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_places(places, counts, causes, visit_types):\n",
    "    indices = []\n",
    "    for i in range(len(places)):\n",
    "        one_visit = places.iloc[i]\n",
    "        indices.append(df_to_grid_index(one_visit, counts, causes, visit_types))\n",
    "    return indices\n",
    "\n",
    "def df_to_grid_index(one_visit, counts, causes, visit_types):\n",
    "    index = 0\n",
    "    p_age = Patient.age_category(one_visit['age'])\n",
    "    index += counts[0]\n",
    "    p_sex = Patient.sex_category(one_visit['sex']) + index\n",
    "    index += counts[1]\n",
    "    p_infection_case = Patient.infection_case_category(one_visit['infection_case'], causes) + index\n",
    "    index += counts[2]\n",
    "    p_type = Patient.type_category(one_visit['type'], visit_types) + index\n",
    "    index += counts[3]\n",
    "    p_date = Patient.day_category(one_visit['date']) + index\n",
    "    row = one_visit['row']\n",
    "    col = one_visit['col']\n",
    "    \n",
    "    return [p_age, p_sex, p_infection_case, p_type, p_date, row, col]\n",
    "\n",
    "def put_triangular_kernel(array, row, col, value, depth):\n",
    "    stride = int((depth - 1) / 2)\n",
    "    ratio = 1 / (stride + 1)\n",
    "    \n",
    "    c = 1\n",
    "    \n",
    "    for i in range(stride, 0, -1):\n",
    "        if row - i >=0 and row - i < array.shape[0] and col-i>=0 and col-i<array.shape[1]:\n",
    "            array[row - i][col-i] += (c * ratio * value)\n",
    "\n",
    "        if row - i >= 0 and row-i<array.shape[0] and col+i <array.shape[1]:\n",
    "            array[row - i][col+i] += (c * ratio * value)\n",
    "            \n",
    "        if row + i < array.shape[0] and row + i >= 0 and col-i >= 0 :\n",
    "            array[row + i][col-i] += (c * ratio * value)\n",
    "            \n",
    "        if row + i < array.shape[0] and row + i >= 0 and col+i <array.shape[1]:\n",
    "            array[row + i][col+i] += (c * ratio * value)\n",
    "            \n",
    "        for j in range(col-i + 1, col+i):\n",
    "            if row-i >= 0 and j >= 0 and row-i<array.shape[0] and j<array.shape[1]:\n",
    "                array[row-i][j] += (c * ratio * value)\n",
    "            if j >= 0 and j < array.shape[1] and row+i<array.shape[0]:\n",
    "                array[row+i][j] += (c * ratio * value)\n",
    "\n",
    "        for j in range(row+i - 1, row-i, -1):\n",
    "            if col + i >= 0 and col+i<array.shape[1] and j >=0 and j<array.shape[1]:\n",
    "                array[j][col + i] += (c * ratio * value)\n",
    "            if col - i >= 0 and col-i<array.shape[1] and j<array.shape[1] and j>=0:\n",
    "                array[j][col - i] += (c * ratio * value)\n",
    "        c += 1\n",
    "    \n",
    "    array[row][col] = value\n",
    "    \n",
    "    return array\n",
    "\n",
    "def overlay_kernel(array):\n",
    "    new_image = np.zeros((array.shape[0],array.shape[1]))\n",
    "    for row in range(array.shape[0]):\n",
    "        for col in range(array.shape[1]):\n",
    "            if array[row][col] == 0: continue\n",
    "            new_image += put_triangular_kernel(np.zeros((array.shape[0], array.shape[1])), row, col, array[row][col], kernel_size)\n",
    "    image_array = new_image\n",
    "        \n",
    "    return new_image\n",
    "\n",
    "def indices_save_image(path, place_indices):\n",
    "    all_counts = sum(count for count in counts)\n",
    "    visit_grid = np.zeros((all_counts, size, size))\n",
    "    \n",
    "    for index in place_indices:\n",
    "        row = index[5]\n",
    "        col = index[6]\n",
    "        for feature in range(5):\n",
    "            visit_grid[index[feature]][row][col] += weight\n",
    "    \n",
    "    for channel in range(visit_grid.shape[0]):\n",
    "        save_grid(path + str(channel) + \".png\", visit_grid[channel])\n",
    "    \n",
    "def save_grid(path, grid):\n",
    "    grid = overlay_kernel(grid)\n",
    "    img = Image.fromarray(grid.astype('uint8'), 'L')\n",
    "    img.save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_patient_route(path, patient, routes):\n",
    "    patient_places = routes[routes['patient_id']==patient]\n",
    "    patient_dates = unique_value(patient_places, 'date')\n",
    "    first_day = datetime.strptime(patient_dates[0], \"%Y-%m-%d\")\n",
    "    last_day = datetime.strptime(patient_dates[-1], \"%Y-%m-%d\") # + timedelta(days=3)\n",
    "    delta = last_day - first_day\n",
    "    duration = delta.days + 1\n",
    "\n",
    "    # 저장\n",
    "    patient_path = path + str(patient)\n",
    "    Path(patient_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    today = first_day\n",
    "    while(True):\n",
    "        today_str = datetime.strftime(today, \"%Y-%m-%d\")\n",
    "        patient_day_places = patient_places[patient_places['date']==today_str]\n",
    "        places_indices = combine_places(patient_day_places, counts, causes, visit_types)\n",
    "        patient_date_path = patient_path + \"/\" + today_str + '/'\n",
    "        Path(patient_date_path).mkdir(parents=True, exist_ok=True)\n",
    "        indices_save_image(patient_date_path, places_indices)\n",
    "        if today == last_day: break\n",
    "        today += timedelta(days=1)\n",
    "\n",
    "def get_patient_route(patient, routes):\n",
    "    patient_places = routes[routes['patient_id']==patient]\n",
    "    patient_dates = unique_value(patient_places, 'date')\n",
    "    first_day = datetime.strptime(patient_dates[0], \"%Y-%m-%d\")\n",
    "    last_day = datetime.strptime(patient_dates[-1], \"%Y-%m-%d\") # + timedelta(days=3)\n",
    "    delta = last_day - first_day\n",
    "    duration = delta.days + 1\n",
    "\n",
    "    patient_routes = []\n",
    "    today = first_day\n",
    "    while(True):\n",
    "        today_str = datetime.strftime(today, \"%Y-%m-%d\")\n",
    "        patient_day_places = patient_places[patient_places['date']==today_str]\n",
    "        places_indices = combine_places(patient_day_places, counts, causes, visit_types)\n",
    "        patient_routes.append([today_str, places_indices]) \n",
    "\n",
    "        if today == last_day: break\n",
    "        today += timedelta(days=1)\n",
    "        \n",
    "    return patient_routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 확진자 기준으로 raw data 저장\n",
    "```\n",
    "path = 'covid_images/patient_figure_raw_64_30_3/'\n",
    "for patient in patients:\n",
    "    save_patient_route(path, patient, routes)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3일 누적 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_two_days(day1, day2):\n",
    "    day2[1].extend(day1[1])\n",
    "    return day2\n",
    "    \n",
    "def accumulate_patient(patient, routes):\n",
    "    patient_route = get_patient_route(patient, routes)\n",
    "    patient_days = len(patient_route)\n",
    "\n",
    "    second = patient_days - 1\n",
    "    first = second - 1\n",
    "    for i in range(2 * patient_days - 3):\n",
    "        patient_route[second] = accumulate_two_days(patient_route[first], patient_route[second])\n",
    "        if second - first == 2: second -=1\n",
    "        else: first -= 1\n",
    "    \n",
    "    return patient_route\n",
    "\n",
    "def save_patient_routes(path, patient, patient_routes):\n",
    "    path += str(patient) + '/'\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    for routes in patient_routes:\n",
    "        patient_date_path = path + routes[0] + '/'\n",
    "        Path(patient_date_path).mkdir(parents=True, exist_ok=True)\n",
    "        indices_save_image(patient_date_path, routes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 누적 경로 저장\n",
    "```\n",
    "path = 'covid_images/patient_figure_accumulated_64_30_3/'\n",
    "for patient in patients:\n",
    "    accumulated_routes = accumulate_patient(patient, routes)\n",
    "    save_patient_routes(path, patient, accumulated_routes)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 날짜별 취합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complete_routes(routes, dates, patients):\n",
    "    first_day = datetime.strptime(dates[0], \"%Y-%m-%d\")\n",
    "    last_day = datetime.strptime(dates[-1], \"%Y-%m-%d\") # + timedelta(days=3)\n",
    "\n",
    "    # 날짜별 경로 배열 생성\n",
    "    today = first_day\n",
    "    complete_routes = []\n",
    "    while(True):\n",
    "        today_str = datetime.strftime(today, \"%Y-%m-%d\")\n",
    "        places = []\n",
    "        complete_routes.append([today_str, places])\n",
    "\n",
    "        if today == last_day: break\n",
    "        today += timedelta(days=1)\n",
    "\n",
    "    # 환자 경로 가져온 다음 날짜대로 배치\n",
    "    for patient in patients:\n",
    "        accumulated_routes = accumulate_patient(patient, routes)\n",
    "        for each_route in accumulated_routes:\n",
    "            route_day = datetime.strptime(each_route[0], \"%Y-%m-%d\")\n",
    "            route_places = each_route[1]\n",
    "            index = (route_day - first_day).days\n",
    "            complete_routes[index][1].extend(route_places)\n",
    "    return complete_routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 취합한 경로 저장\n",
    "```\n",
    "complete_routes = get_complete_routes(routes, dates, patients)\n",
    "\n",
    "path = 'covid_images/complete_figure_kernel_64_10_5/'\n",
    "Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for days in complete_routes:\n",
    "    date_path = path + days[0] + '/'\n",
    "    Path(date_path).mkdir(parents=True, exist_ok=True)\n",
    "    indices_save_image(date_path, days[1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 아직 하는 중\n",
    "```\n",
    "def get_accumulated_routes_by_date(routes, dates, patients):\n",
    "    first_day = datetime.strptime(dates[0], \"%Y-%m-%d\")\n",
    "    last_day = datetime.strptime(dates[-1], \"%Y-%m-%d\")\n",
    "\n",
    "    # 날짜별 경로 배열 생성\n",
    "    today = first_day\n",
    "    complete_routes = []\n",
    "    while(True):\n",
    "        today_str = datetime.strftime(today, \"%Y-%m-%d\")\n",
    "        ids = []\n",
    "        places = []\n",
    "        complete_routes.append([today_str, []])\n",
    "        if today == last_day: break\n",
    "        today += timedelta(days=1)\n",
    "\n",
    "    for patient in patients:\n",
    "        accumulated_routes = accumulate_patient(patient, routes)\n",
    "        for each_route in accumulated_routes:\n",
    "            route_day = datetime.strptime(each_route[0], \"%Y-%m-%d\")\n",
    "            route_places = each_route[1]\n",
    "            index = (route_day - first_day).days\n",
    "            complete_routes[index][1].append([patient, route_places])\n",
    "    return complete_routes\n",
    "\n",
    "accumulated_routes = get_accumulated_routes_by_date(routes, dates, patients)\n",
    "\n",
    "path = 'covid_images/patient_accumulated_by_date/'\n",
    "Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for days in accumulated_routes:\n",
    "    date_path = path + days[0] + '/'\n",
    "    Path(date_path).mkdir(parents=True, exist_ok=True)\n",
    "    for day in days[1]:\n",
    "        patient_id = day[0]\n",
    "        routes = day[1]\n",
    "        day_path = date_path + str(patient_id) + '/'\n",
    "        Path(day_path).mkdir(parents=True, exist_ok=True)\n",
    "        indices_save_image(day_path, routes)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(routes, dates, patients):\n",
    "    complete_routes = get_complete_routes(routes, dates, patients)\n",
    "    all_counts = sum(count for count in counts)\n",
    "    dataset = np.zeros((len(complete_routes), all_counts, size, size))\n",
    "\n",
    "    for i, days in enumerate(complete_routes):\n",
    "        sub_routes = get_array_image(days, dataset[i,:,:,:])\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def get_array_image(place_indices, data_array):\n",
    "    all_counts = sum(count for count in counts)\n",
    "    \n",
    "    for index in place_indices[1]:\n",
    "        row = index[5]\n",
    "        col = index[6]\n",
    "        for feature in range(5):\n",
    "            data_array[index[feature]][row][col] += weight\n",
    "    \n",
    "    for channel in range(data_array.shape[0]):\n",
    "        data_array[channel] = overlay_kernel(data_array[channel])\n",
    "    \n",
    "    return data_array\n",
    "    \n",
    "def split_dataset(dataset, n_train, n_step):\n",
    "    n = dataset.shape[0]\n",
    "    channel = dataset.shape[1]\n",
    "\n",
    "    n_train = int(n * 0.7) if n_train == 0 else n_train\n",
    "    n_test = n - n_train\n",
    "\n",
    "    train = dataset[:n_train,:,:]\n",
    "    test = dataset[n_train:,:,:]\n",
    "\n",
    "    # train set\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for i in range(n_step, n_train-n_step + 1):\n",
    "        X_train.append(train[i-n_step:i, :,:])\n",
    "        y_train.append(train[i:i+n_step, :,:])\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "   \n",
    "    \n",
    "    # test set\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for i in range(n_step,n_test):\n",
    "        X_test.append(train[i-n_step:i, :,:])\n",
    "        y_test.append(train[i:i+1, :,:])\n",
    "    X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "   \n",
    "    return X_train, y_train, X_test, y_test\n",
    "    \n",
    "def save_dataset(path, X_train, y_train, X_test, y_test):\n",
    "    with h5py.File(path,'w') as f:    \n",
    "        train_X = f.create_dataset('X_train', data=X_train)\n",
    "        train_y = f.create_dataset('y_train', data=y_train)\n",
    "        test_X = f.create_dataset('X_test', data=X_test)\n",
    "        test_y = f.create_dataset('y_test', data=y_test)\n",
    "    \n",
    "def load_data(path):\n",
    "    X_train = HDF5Matrix(path, 'X_train')\n",
    "    y_train = HDF5Matrix(path, 'y_train')\n",
    "    X_test = HDF5Matrix(path, 'X_test')\n",
    "    y_test = HDF5Matrix(path, 'y_test')\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save dataset\n",
    "```\n",
    "dataset = get_dataset(routes, dates, patients)\n",
    "X_train, y_train, X_test, y_test = split_dataset(dataset, 60, 3)\n",
    "save_dataset(dataset_path , X_train, y_train, X_test, y_test)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.convolutional import Conv3D\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as tfback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = 256\n",
    "channel = 45\n",
    "n_step = 3\n",
    "n_test = 3\n",
    "epochs = 50\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a layer which take as input movies of shape\n",
    "# (n_frames, width, height, channels) and returns a movie\n",
    "# of identical shape.\n",
    "def get_model(channel):\n",
    "    with K.tf_ops.device('/GPU:0'):\n",
    "        seq = Sequential()\n",
    "        seq.add(ConvLSTM2D(filters=channel, kernel_size=(3, 3), data_format='channels_first',\n",
    "                           input_shape=(n_step, channel, rs, rs),\n",
    "                           padding='same', return_sequences=True))\n",
    "        seq.add(BatchNormalization())\n",
    "\n",
    "        seq.add(ConvLSTM2D(filters=channel, kernel_size=(3, 3), data_format='channels_first',\n",
    "                           padding='same', return_sequences=True))\n",
    "        seq.add(BatchNormalization())\n",
    "\n",
    "        seq.add(ConvLSTM2D(filters=channel, kernel_size=(3, 3), data_format='channels_first',\n",
    "                           padding='same', return_sequences=True))\n",
    "        seq.add(BatchNormalization())\n",
    "\n",
    "        seq.add(ConvLSTM2D(filters=channel, kernel_size=(3, 3), data_format='channels_first',\n",
    "                           padding='same', return_sequences=True))\n",
    "        seq.add(BatchNormalization())\n",
    "\n",
    "        seq.add(Conv3D(filters=n_step, kernel_size=(3, 3, 3),\n",
    "                       activation='sigmoid',\n",
    "                       padding='same', data_format='channels_first'))\n",
    "\n",
    "        seq.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "                \n",
    "        seq.summary()\n",
    "\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array_image(place_indices, data_array):\n",
    "    all_counts = sum(count for count in counts)\n",
    "    \n",
    "    for index in place_indices[1]:\n",
    "        row = index[5]\n",
    "        col = index[6]\n",
    "        for feature in range(5):\n",
    "            data_array[index[feature]][row][col] += weight\n",
    "    \n",
    "    for channel in range(data_array.shape[0]):\n",
    "        data_array[channel] = overlay_kernel(data_array[channel])\n",
    "    \n",
    "    return data_array\n",
    "\n",
    "def train(X_train, y_train):\n",
    "    seq = get_model(channel)\n",
    "    seq.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, shuffle='batch')\n",
    "    seq.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yejin/.conda/envs/devol/lib/python3.6/site-packages/keras/utils/io_utils.py:60: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  f = h5py.File(datapath)\n"
     ]
    }
   ],
   "source": [
    "#### 데이터셋 불러오기\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_lst_m2d_1 (ConvLSTM2D)  (None, 3, 45, 256, 256)   145980    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 3, 45, 256, 256)   1024      \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_2 (ConvLSTM2D)  (None, 3, 45, 256, 256)   145980    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 3, 45, 256, 256)   1024      \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_3 (ConvLSTM2D)  (None, 3, 45, 256, 256)   145980    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 3, 45, 256, 256)   1024      \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_4 (ConvLSTM2D)  (None, 3, 45, 256, 256)   145980    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 3, 45, 256, 256)   1024      \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 3, 45, 256, 256)   246       \n",
      "=================================================================\n",
      "Total params: 588,262\n",
      "Trainable params: 586,214\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[1,45,256,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node conv_lst_m2d_3/while/body/_569/Mul_1}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_keras_scratch_graph_10257]\n\nFunction call stack:\nkeras_scratch_graph\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-372b69cf3823>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#### 트레이닝 함수 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-6af50277a824>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X_train, y_train)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/devol/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/.conda/envs/devol/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/devol/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3727\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/devol/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/devol/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/devol/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/devol/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.conda/envs/devol/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[0;32m~/.conda/envs/devol/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[1,45,256,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node conv_lst_m2d_3/while/body/_569/Mul_1}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_keras_scratch_graph_10257]\n\nFunction call stack:\nkeras_scratch_graph\n"
     ]
    }
   ],
   "source": [
    "#### 트레이닝 함수 실행\n",
    "\n",
    "train(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### predict 함수 실행\n",
    "\n",
    "model = load_model(model_path)\n",
    "pred =model.predict(X_test)\n",
    "print(pred.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
